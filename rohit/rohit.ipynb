{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import numpy\n",
    "\n",
    "def LDA(data) :\n",
    "\n",
    "   # list for tokenized documents in loop\n",
    "   texts = []\n",
    "\n",
    "   # create English stop words list\n",
    "   en_stop = stopwords.words('english')\n",
    "\n",
    "   ## Tokenizing\n",
    "   tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "   # create English stop words list\n",
    "   en_stop = stopwords.words('english')\n",
    "\n",
    "   # loop through document list\n",
    "   tokens = tokenizer.tokenize(data)\n",
    "   stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "   texts.append(stopped_tokens)\n",
    "\n",
    "   # turn our tokenized documents into a id <-> term dictionary\n",
    "   dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "   # convert tokenized documents into a document-term matrix\n",
    "   corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "   ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=100)\n",
    "   #ldamodel.print_topics(num_topics=1, num_words=20)\n",
    "   list = ldamodel.show_topics(num_topics=20, formatted=False)\n",
    "   topics = []\n",
    "   for i in list[0][1]:\n",
    "     topics.append(i[0])\n",
    "   return topics\n",
    "\n",
    "\n",
    "def summarize(data, top_sentences, n=400, cluster_threshold=8):\n",
    "    print (data)\n",
    "    # Adapted from \"The Automatic Creation of Literature Abstracts\" by H.P. Luhn\n",
    "    #\n",
    "    # Parameters:\n",
    "    # * n  - Number of words to consider\n",
    "    # * cluster_threshold - Distance between words to consider\n",
    "    # * top_sentences - Number of sentences to return for a \"top n\" summary\n",
    "            \n",
    "    # Begin - nested helper function\n",
    "    def score_sentences(sentences, important_words):\n",
    "        scores = []\n",
    "        sentence_idx = -1\n",
    "    \n",
    "        for s in [nltk.tokenize.word_tokenize(s) for s in sentences]:\n",
    "    \n",
    "            sentence_idx += 1\n",
    "            word_idx = []\n",
    "    \n",
    "            # For each word in the word list...\n",
    "            for w in important_words:\n",
    "                try:\n",
    "                    # Compute an index for important words in each sentence\n",
    "                    word_idx.append(s.index(w))\n",
    "                except ValueError as exc: # w not in this particular sentence\n",
    "                    pass\n",
    "    \n",
    "            word_idx.sort()\n",
    "    \n",
    "            # It is possible that some sentences may not contain any important words\n",
    "            if len(word_idx)== 0: continue\n",
    "    \n",
    "            # Using the word index, compute clusters with a max distance threshold\n",
    "            # for any two consecutive words\n",
    "    \n",
    "            clusters = []\n",
    "            cluster = [word_idx[0]]\n",
    "            i = 1\n",
    "            while i < len(word_idx):\n",
    "                if word_idx[i] - word_idx[i - 1] < cluster_threshold:\n",
    "                    cluster.append(word_idx[i])\n",
    "                else:\n",
    "                    clusters.append(cluster[:])\n",
    "                    cluster = [word_idx[i]]\n",
    "                i += 1\n",
    "            clusters.append(cluster)\n",
    "    \n",
    "            # Score each cluster. The max score for any given cluster is the score \n",
    "            # for the sentence.\n",
    "    \n",
    "            max_cluster_score = 0\n",
    "            for c in clusters:\n",
    "                significant_words_in_cluster = len(c)\n",
    "                total_words_in_cluster = c[-1] - c[0] + 1\n",
    "                score = 1.0 * significant_words_in_cluster \\\n",
    "                    * significant_words_in_cluster / total_words_in_cluster\n",
    "    \n",
    "                if score > max_cluster_score:\n",
    "                    max_cluster_score = score\n",
    "    \n",
    "            scores.append((sentence_idx, score))\n",
    "    \n",
    "        return scores    \n",
    "    \n",
    "\n",
    "    # It's entirely possible that this \"clean page\" will be a big mess. YMMV.\n",
    "    # The good news is that the summarize algorithm inherently accounts for handling\n",
    "    # a lot of this noise.\n",
    "   \n",
    "    sentences = [s for s in nltk.tokenize.sent_tokenize(data)]\n",
    "    normalized_sentences = [s.lower() for s in sentences]\n",
    "\n",
    "    words = [w.lower() for sentence in normalized_sentences for w in\n",
    "             nltk.tokenize.word_tokenize(sentence)]\n",
    "\n",
    "    top_n_words = LDA(data)\n",
    "    print (top_n_words)\n",
    "    scored_sentences = score_sentences(normalized_sentences, top_n_words)\n",
    "\n",
    "    # approach would be to return only the top N ranked sentences\n",
    "\n",
    "    top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-top_sentences:]\n",
    "    top_n_scored = sorted(top_n_scored, key=lambda s: s[0])\n",
    "\n",
    "    # Decorate the post object with summaries\n",
    "    return dict(top_n_summary=[sentences[idx] for (idx, score) in top_n_scored])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:9000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [18/Feb/2019 12:21:58] \"GET / HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [18/Feb/2019 12:21:59] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/env python\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "#import summarization\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/textsummarization', methods=['GET'])\n",
    "def classify():\n",
    "   return render_template('summarization.html')\n",
    "\n",
    "# service use for summarize the given text\n",
    "@app.route('/summarize', methods=['POST'])\n",
    "def summarizer():\n",
    "    response = None\n",
    "    if request.method == 'POST' :\n",
    "        try:\n",
    "            req_data = request.get_json()\n",
    "            print (req_data)\n",
    "            print (req_data['top_sentences'])\n",
    "            response = summarize(req_data['data'],int(req_data['top_sentences']))\n",
    "        except Exception as e:\n",
    "            return respond(e)\n",
    "       \n",
    "    return respond(None, res=response)\n",
    "       \n",
    "def respond(err, res=None):\n",
    "    return_res =  {\n",
    "        'status_code': 400 if err else 200,\n",
    "        'body': err.message if err else res,\n",
    "    }\n",
    "    return jsonify(return_res)\n",
    "\n",
    "# start the server with the 'run()' method\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
