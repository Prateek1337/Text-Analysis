{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample text:\n",
    "\n",
    "my_text=\"\"\"Warren McCulloch and Walter Pitts[3] (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.[4]\n",
    "Hebbian learning\n",
    "\n",
    "In the late 1940s, D. O. Hebb[5] created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines. Farley and Clark[6] (1954) first used computational machines, then called \"calculators\", to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).[7] Rosenblatt[8] (1958) created the perceptron, an algorithm for pattern recognition. With mathematical notation, Rosenblatt described circuitry not in the basic perceptron, such as the exclusive-or circuit that could not be processed by neural networks at the time.[9] In 1959, a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.[10] The first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, becoming the Group Method of Data Handling.[11][12][13]\n",
    "\n",
    "Neural network research stagnated after machine learning research by Minsky and Papert (1969),[14] who discovered two key issues with the computational machines that processed neural networks. The first was that basic perceptrons were incapable of processing the exclusive-or circuit. The second was that computers didn't have enough processing power to effectively handle the work required by large neural networks. Neural network research slowed until computers achieved far greater processing power. Much of artificial intelligence had focused on high-level (symbolic) models that are processed by using algorithms, characterized for example by expert systems with knowledge embodied in if-then rules, until in the late 1980s research expanded to low-level (sub-symbolic) machine learning, characterized by knowledge embodied in the parameters of a cognitive model.[citation needed]\n",
    "Backpropagation\n",
    "\n",
    "A key trigger for renewed interest in neural networks and learning was Werbos's (1975) backpropagation algorithm that effectively solved the exclusive-or problem by making the training of multi-layer networks feasible and efficient. Backpropagation distributed the error term back up through the layers, by modifying the weights at each node.[9]\n",
    "\n",
    "In the mid-1980s, parallel distributed processing became popular under the name connectionism. Rumelhart and McClelland (1986) described the use of connectionism to simulate neural processes.[15]\n",
    "\n",
    "Support vector machines and other, much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity. However, using neural networks transformed some domains, such as the prediction of protein structures.[16][17]\n",
    "\n",
    "In 1992, max-pooling was introduced to help with least shift invariance and tolerance to deformation to aid in 3D object recognition.[18][19][20] In 2010, Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants.[21]\n",
    "\n",
    "The vanishing gradient problem affects many-layered feedforward networks that used backpropagation and also recurrent neural networks (RNNs).[22][23] As errors propagate from layer to layer, they shrink exponentially with the number of layers, impeding the tuning of neuron weights that is based on those errors, particularly affecting deep networks.\n",
    "\n",
    "To overcome this problem, Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation.[24] Behnke (2003) relied only on the sign of the gradient (Rprop)[25] on problems such as image reconstruction and face localization.\n",
    "\n",
    "Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine[26] to model each layer. Once sufficiently many layers have been learned, the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an \"ancestral pass\") from the top level feature activations.[27][28] In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.[29]\n",
    "\n",
    "Earlier challenges in training deep neural networks were successfully addressed with methods such as unsupervised pre-training, while available computing power increased through the use of GPUs and distributed computing. Neural networks were deployed on a large scale, particularly in image and visual recognition problems. This became known as \"deep learning\".[citation needed]\n",
    "Hardware-based designs\n",
    "\n",
    "Computational devices were created in CMOS, for both biophysical simulation and neuromorphic computing. Nanodevices[30] for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital (even though the first implementations may use digital devices).[31] Ciresan and colleagues (2010)[32] in Schmidhuber's group showed that despite the vanishing gradient problem, GPUs makes back-propagation feasible for many-layered feedforward neural networks.\n",
    "Contests\n",
    "\n",
    "Between 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group won eight international competitions in pattern recognition and machine learning.[33][34] For example, the bi-directional and multi-dimensional long short-term memory (LSTM)[35][36][37][38] of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three languages to be learned.[37][36]\n",
    "\n",
    "Ciresan and colleagues won pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition,[39] the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge[40] and others. Their neural networks were the first pattern recognizers to achieve human-competitive or even superhuman performance[41] on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem.\n",
    "\n",
    "Researchers demonstrated (2010) that deep neural networks interfaced to a hidden Markov model with context-dependent states that define the neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search.\n",
    "\n",
    "GPU-based implementations[42] of this approach won many pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition,[39] the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge,[40] the ImageNet Competition[43] and others.\n",
    "\n",
    "Deep, highly nonlinear neural architectures similar to the neocognitron[44] and the \"standard architecture of vision\",[45] inspired by simple and complex cells, were pre-trained by unsupervised methods by Hinton.[46][27] A team from his lab won a 2012 contest sponsored by Merck to design software to help find molecules that might identify new drugs.[47]\n",
    "Convolutional networks\n",
    "\n",
    "As of 2011, the state of the art in deep learning feedforward networks alternated between convolutional layers and max-pooling layers,[42][48] topped by several fully or sparsely connected layers followed by a final classification layer. Learning is usually done without unsupervised pre-training. In the convolutional layer, there are filters that are convolved with the input. Each filter is equivalent to a weights vector that has to be trained.\n",
    "\n",
    "Such supervised deep learning methods were the first to achieve human-competitive performance on certain tasks.[41]\n",
    "\n",
    "Artificial neural networks were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes, only when invariance extended beyond shift, to all ANN-learned concepts, such as location, type (object class label), scale, lighting and others. This was realized in Developmental Networks (DNs)[49] whose embodiments are Where-What Networks, WWN-1 (2008)[50] through WWN-7 (2013).[51] \" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Convolutional networks\n",
      "\n",
      "As of 2011, the state of the art in deep learning feedforward networks alternated between convolutional layers and max-pooling layers,   topped by several fully or sparsely connected layers followed by a final classification layer.\n",
      "Hebbian learning\n",
      "\n",
      "In the late 1940s, D. O. Hebb  created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning.\n",
      "Contests\n",
      "\n",
      "Between 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group won eight international competitions in pattern recognition and machine learning.\n",
      "Researchers demonstrated (2010) that deep neural networks interfaced to a hidden Markov model with context-dependent states that define the neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search.\n",
      "Neural network research stagnated after machine learning research by Minsky and Papert (1969),  who discovered two key issues with the computational machines that processed neural networks.\n",
      "Process ended:  2019-03-14 21:45:09\n",
      "Total time required for  1  articles to be summarized:  0.034 seconds\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from datetime import datetime\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "try:\n",
    "    import urllib.request as urllib2\n",
    "except ImportError:\n",
    "    import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "time1 = time.time()\n",
    "\n",
    "\n",
    "#Summary maker1:Word frequency model\n",
    "\n",
    "class frequency_summarizer:\n",
    "    def __init__(self, minCut=0.2, maxCut=0.8):\n",
    "        self._min_cut = minCut\n",
    "        self._max_cut = maxCut\n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "    \"\"\"words with higher frequency than maxcut and lower frequency than mincut are eliminated \"\"\"\n",
    "    def _computefrequencies(self, word_sent):\n",
    "\n",
    "        freq = defaultdict(int)\n",
    "        for s in word_sent:\n",
    "            for word in s:\n",
    "                if word not in self._stopwords:\n",
    "                    freq[word] += 1\n",
    "\n",
    "        m = float(max(freq.values()))\n",
    "        print(\"\\n\\n\")\n",
    "        for w in list(freq):\n",
    "#             print(\"word=\",w,\" freq[w]=\",freq[w])\n",
    "            freq[w] = freq[w] / m\n",
    "            if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
    "                del freq[w]\n",
    "        print(\"\\n\\n\")\n",
    "        return freq\n",
    "    \"\"\"calculate frequency for each word \"\"\"\n",
    "    def summarize(self, text, n):\n",
    "        regex = re.compile(\"\\[[0-9a-zA-Z ]+\\]\")\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        sents = sent_tokenize(text)\n",
    "        assert n <= len(sents)\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "        self._freq = self._computefrequencies(word_sent)\n",
    "        ranking = defaultdict(int)\n",
    "        for i, sent in enumerate(word_sent):\n",
    "            for w in sent:\n",
    "                if w in self._freq:\n",
    "                    ranking[i] += self._freq[w]\n",
    "        sents_idx = self._rank(ranking, n)\n",
    "        return [sents[j] for j in sents_idx]\n",
    "    \"\"\" Print first n sentences which represent summary \"\"\"\n",
    "    \n",
    "\n",
    "    def _rank(self, ranking, n):\n",
    "        return nlargest(n, ranking, key=ranking.get)\n",
    "\n",
    "def get_only_text(url):\n",
    "    \"\"\"\n",
    "  print title and text of article in URL\n",
    " \"\"\"\n",
    "    page = urllib2.urlopen(url).read().decode('utf8')\n",
    "    soup = BeautifulSoup(page)\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
    "    return soup.title.text, text\n",
    "\n",
    "\n",
    "no_of_docs=1\n",
    "fs = frequency_summarizer()\n",
    "for s in fs.summarize(my_text, 5):\n",
    "    print (s)\n",
    "time2 = time.time()\n",
    "total_time=(time2 - time1)\n",
    "ind_time=(total_time / no_of_docs)\n",
    "print (\"Process ended: \", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print (\"Total time required for \", no_of_docs, \" articles to be summarized: \", round(total_time,3) , \"seconds\")\n",
    "\n",
    "#Summary1,  number of sentences=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text=\"\"\"Warren McCulloch and Walter Pitts[3] (1943) created a computational model for neural networks based on mathematics and algorithms called threshold logic. This model paved the way for neural network research to split into two approaches. One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.[4]\n",
    "Hebbian learning\n",
    "\n",
    "In the late 1940s, D. O. Hebb[5] created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines. Farley and Clark[6] (1954) first used computational machines, then called \"calculators\", to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).[7] Rosenblatt[8] (1958) created the perceptron, an algorithm for pattern recognition. With mathematical notation, Rosenblatt described circuitry not in the basic perceptron, such as the exclusive-or circuit that could not be processed by neural networks at the time.[9] In 1959, a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.[10] The first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, becoming the Group Method of Data Handling.[11][12][13]\n",
    "\n",
    "Neural network research stagnated after machine learning research by Minsky and Papert (1969),[14] who discovered two key issues with the computational machines that processed neural networks. The first was that basic perceptrons were incapable of processing the exclusive-or circuit. The second was that computers didn't have enough processing power to effectively handle the work required by large neural networks. Neural network research slowed until computers achieved far greater processing power. Much of artificial intelligence had focused on high-level (symbolic) models that are processed by using algorithms, characterized for example by expert systems with knowledge embodied in if-then rules, until in the late 1980s research expanded to low-level (sub-symbolic) machine learning, characterized by knowledge embodied in the parameters of a cognitive model.[citation needed]\n",
    "Backpropagation\n",
    "\n",
    "A key trigger for renewed interest in neural networks and learning was Werbos's (1975) backpropagation algorithm that effectively solved the exclusive-or problem by making the training of multi-layer networks feasible and efficient. Backpropagation distributed the error term back up through the layers, by modifying the weights at each node.[9]\n",
    "\n",
    "In the mid-1980s, parallel distributed processing became popular under the name connectionism. Rumelhart and McClelland (1986) described the use of connectionism to simulate neural processes.[15]\n",
    "\n",
    "Support vector machines and other, much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity. However, using neural networks transformed some domains, such as the prediction of protein structures.[16][17]\n",
    "\n",
    "In 1992, max-pooling was introduced to help with least shift invariance and tolerance to deformation to aid in 3D object recognition.[18][19][20] In 2010, Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants.[21]\n",
    "\n",
    "The vanishing gradient problem affects many-layered feedforward networks that used backpropagation and also recurrent neural networks (RNNs).[22][23] As errors propagate from layer to layer, they shrink exponentially with the number of layers, impeding the tuning of neuron weights that is based on those errors, particularly affecting deep networks.\n",
    "\n",
    "To overcome this problem, Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation.[24] Behnke (2003) relied only on the sign of the gradient (Rprop)[25] on problems such as image reconstruction and face localization.\n",
    "\n",
    "Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine[26] to model each layer. Once sufficiently many layers have been learned, the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an \"ancestral pass\") from the top level feature activations.[27][28] In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.[29]\n",
    "\n",
    "Earlier challenges in training deep neural networks were successfully addressed with methods such as unsupervised pre-training, while available computing power increased through the use of GPUs and distributed computing. Neural networks were deployed on a large scale, particularly in image and visual recognition problems. This became known as \"deep learning\".[citation needed]\n",
    "Hardware-based designs\n",
    "\n",
    "Computational devices were created in CMOS, for both biophysical simulation and neuromorphic computing. Nanodevices[30] for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital (even though the first implementations may use digital devices).[31] Ciresan and colleagues (2010)[32] in Schmidhuber's group showed that despite the vanishing gradient problem, GPUs makes back-propagation feasible for many-layered feedforward neural networks.\n",
    "Contests\n",
    "\n",
    "Between 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group won eight international competitions in pattern recognition and machine learning.[33][34] For example, the bi-directional and multi-dimensional long short-term memory (LSTM)[35][36][37][38] of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three languages to be learned.[37][36]\n",
    "\n",
    "Ciresan and colleagues won pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition,[39] the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge[40] and others. Their neural networks were the first pattern recognizers to achieve human-competitive or even superhuman performance[41] on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem.\n",
    "\n",
    "Researchers demonstrated (2010) that deep neural networks interfaced to a hidden Markov model with context-dependent states that define the neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search.\n",
    "\n",
    "GPU-based implementations[42] of this approach won many pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition,[39] the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge,[40] the ImageNet Competition[43] and others.\n",
    "\n",
    "Deep, highly nonlinear neural architectures similar to the neocognitron[44] and the \"standard architecture of vision\",[45] inspired by simple and complex cells, were pre-trained by unsupervised methods by Hinton.[46][27] A team from his lab won a 2012 contest sponsored by Merck to design software to help find molecules that might identify new drugs.[47]\n",
    "Convolutional networks\n",
    "\n",
    "As of 2011, the state of the art in deep learning feedforward networks alternated between convolutional layers and max-pooling layers,[42][48] topped by several fully or sparsely connected layers followed by a final classification layer. Learning is usually done without unsupervised pre-training. In the convolutional layer, there are filters that are convolved with the input. Each filter is equivalent to a weights vector that has to be trained.\n",
    "\n",
    "Such supervised deep learning methods were the first to achieve human-competitive performance on certain tasks.[41]\n",
    "\n",
    "Artificial neural networks were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes, only when invariance extended beyond shift, to all ANN-learned concepts, such as location, type (object class label), scale, lighting and others. This was realized in Developmental Networks (DNs)[49] whose embodiments are Where-What Networks, WWN-1 (2008)[50] through WWN-7 (2013).[51] \" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Warren McCulloch and Walter Pitts    created a computational model for neural networks based on mathematics and algorithms called threshold logic.',\n",
       " 'This model paved the way for neural network research to split into two approaches.',\n",
       " 'One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence.',\n",
       " 'This work led to work on nerve networks and their link to finite automata.',\n",
       " 'Hebbian learning In the late  s  D. O. Hebb  created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning.',\n",
       " 'Hebbian learning is unsupervised learning.',\n",
       " 'This evolved into models for long term potentiation.',\n",
       " 'Researchers started applying these ideas to computational models in   with Turing s Btype machines.',\n",
       " 'Farley and Clark    first used computational machines  then called  calculators  to simulate a Hebbian network.',\n",
       " 'Other neural network computational machines were created by Rochester  Holland  Habit and Duda  .',\n",
       " 'Rosenblatt    created the perceptron  an algorithm for pattern recognition.',\n",
       " 'With mathematical notation  Rosenblatt described circuitry not in the basic perceptron  such as the exclusiveor circuit that could not be processed by neural networks at the time.',\n",
       " 'In   a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex  simple cells and complex cells.',\n",
       " 'The first functional networks with many layers were published by Ivakhnenko and Lapa in   becoming the Group Method of Data Handling.',\n",
       " 'Neural network research stagnated after machine learning research by Minsky and Papert   who discovered two key issues with the computational machines that processed neural networks.',\n",
       " 'The first was that basic perceptrons were incapable of processing the exclusiveor circuit.',\n",
       " 'The second was that computers didn t have enough processing power to effectively handle the work required by large neural networks.',\n",
       " 'Neural network research slowed until computers achieved far greater processing power.',\n",
       " 'Much of artificial intelligence had focused on highlevel  symbolic  models that are processed by using algorithms  characterized for example by expert systems with knowledge embodied in ifthen rules  until in the late  s research expanded to lowlevel  subsymbolic  machine learning  characterized by knowledge embodied in the parameters of a cognitive model.',\n",
       " 'citation needed Backpropagation A key trigger for renewed interest in neural networks and learning was Werbos s   backpropagation algorithm that effectively solved the exclusiveor problem by making the training of multilayer networks feasible and efficient.',\n",
       " 'Backpropagation distributed the error term back up through the layers  by modifying the weights at each node.',\n",
       " 'In the mid s  parallel distributed processing became popular under the name connectionism.',\n",
       " 'Rumelhart and McClelland   described the use of connectionism to simulate neural processes.',\n",
       " 'Support vector machines and other  much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity.',\n",
       " 'However  using neural networks transformed some domains  such as the prediction of protein structures.',\n",
       " 'In   maxpooling was introduced to help with least shift invariance and tolerance to deformation to aid in  D object recognition.',\n",
       " 'In   Backpropagation training through maxpooling was accelerated by GPUs and shown to perform better than other pooling variants.',\n",
       " 'The vanishing gradient problem affects manylayered feedforward networks that used backpropagation and also recurrent neural networks  RNNs .',\n",
       " 'As errors propagate from layer to layer  they shrink exponentially with the number of layers  impeding the tuning of neuron weights that is based on those errors  particularly affecting deep networks.',\n",
       " 'To overcome this problem  Schmidhuber adopted a multilevel hierarchy of networks   pretrained one level at a time by unsupervised learning and finetuned by backpropagation.',\n",
       " 'Behnke   relied only on the sign of the gradient  Rprop  on problems such as image reconstruction and face localization.',\n",
       " 'Hinton et al.',\n",
       " 'proposed learning a highlevel representation using successive layers of binary or realvalued latent variables with a restricted Boltzmann machine  to model each layer.',\n",
       " 'Once sufficiently many layers have been learned  the deep architecture may be used as a generative model by reproducing the data when sampling down the model  an  ancestral pass  from the top level feature activations.',\n",
       " 'In   Ng and Dean created a network that learned to recognize higherlevel concepts  such as cats  only from watching unlabeled images taken from YouTube videos.',\n",
       " 'Earlier challenges in training deep neural networks were successfully addressed with methods such as unsupervised pretraining  while available computing power increased through the use of GPUs and distributed computing.',\n",
       " 'Neural networks were deployed on a large scale  particularly in image and visual recognition problems.',\n",
       " 'This became known as  deep learning .',\n",
       " 'citation needed Hardwarebased designs Computational devices were created in CMOS  for both biophysical simulation and neuromorphic computing.',\n",
       " 'Nanodevices  for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital  even though the first implementations may use digital devices .',\n",
       " 'Ciresan and colleagues   in Schmidhuber s group showed that despite the vanishing gradient problem  GPUs makes backpropagation feasible for manylayered feedforward neural networks.',\n",
       " 'Contests Between   and   recurrent neural networks and deep feedforward neural networks developed in Schmidhuber s research group won eight international competitions in pattern recognition and machine learning.',\n",
       " 'For example  the bidirectional and multidimensional long shortterm memory  LSTM  of Graves et al.',\n",
       " 'won three competitions in connected handwriting recognition at the   International Conference on Document Analysis and Recognition  ICDAR  without any prior knowledge about the three languages to be learned.',\n",
       " 'Ciresan and colleagues won pattern recognition contests  including the IJCNN   Traffic Sign Recognition Competition  the ISBI   Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge  and others.',\n",
       " 'Their neural networks were the first pattern recognizers to achieve humancompetitive or even superhuman performance  on benchmarks such as traffic sign recognition  IJCNN   or the MNIST handwritten digits problem.',\n",
       " 'Researchers demonstrated   that deep neural networks interfaced to a hidden Markov model with contextdependent states that define the neural network output layer can drastically reduce errors in largevocabulary speech recognition tasks such as voice search.',\n",
       " 'GPUbased implementations  of this approach won many pattern recognition contests  including the IJCNN   Traffic Sign Recognition Competition  the ISBI   Segmentation of neuronal structures in EM stacks challenge  the ImageNet Competition  and others.',\n",
       " 'Deep  highly nonlinear neural architectures similar to the neocognitron  and the  standard architecture of vision  inspired by simple and complex cells  were pretrained by unsupervised methods by Hinton.',\n",
       " 'A team from his lab won a   contest sponsored by Merck to design software to help find molecules that might identify new drugs.',\n",
       " 'Convolutional networks As of   the state of the art in deep learning feedforward networks alternated between convolutional layers and maxpooling layers  topped by several fully or sparsely connected layers followed by a final classification layer.',\n",
       " 'Learning is usually done without unsupervised pretraining.',\n",
       " 'In the convolutional layer  there are filters that are convolved with the input.',\n",
       " 'Each filter is equivalent to a weights vector that has to be trained.',\n",
       " 'Such supervised deep learning methods were the first to achieve humancompetitive performance on certain tasks.',\n",
       " 'Artificial neural networks were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes  only when invariance extended beyond shift  to all ANNlearned concepts  such as location  type  object class label  scale  lighting and others.',\n",
       " 'This was realized in Developmental Networks  DNs  whose embodiments are WhereWhat Networks  WWN    through WWN   .']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary maker2: TFIDF model\n",
    "\n",
    "my_text = re.sub(\"[^A-Za-z .-]+\", \" \" , my_text)\n",
    "my_text = my_text.replace(\"-\", \"\")\n",
    "my_text = my_text.replace(\"â€¦\", \"\")\n",
    "my_text = my_text.replace(\"Mr.\", \"Mr\").replace(\"Mrs.\", \"Mrs\")\n",
    "sents = sent_tokenize(my_text)\n",
    "sents #processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'about', 'accelerated', 'achieve', 'achieved', 'activations', 'addressed', 'adopted', 'affecting', 'affects', 'after', 'aid', 'al', 'algorithm', 'algorithms', 'all', 'also', 'alternated', 'an', 'analog', 'analyses', 'analysis', 'ancestral', 'and', 'annlearned', 'any', 'application', 'applying', 'approach', 'approaches', 'architecture', 'architectures', 'are', 'art', 'artificial', 'as', 'at', 'automata', 'available', 'back', 'backpropagation', 'based', 'basic', 'be', 'became', 'because', 'becoming', 'been', 'behnke', 'benchmarks', 'better', 'between', 'beyond', 'bidirectional', 'binary', 'biological', 'biophysical', 'boltzmann', 'both', 'brain', 'btype', 'by', 'calculators', 'called', 'can', 'cats', 'cells', 'certain', 'challenge', 'challenges', 'characterized', 'circuit', 'circuitry', 'ciresan', 'citation', 'clark', 'class', 'classification', 'classifiers', 'cluttered', 'cmos', 'cognitive', 'colleagues', 'competition', 'competitions', 'complex', 'components', 'computational', 'computers', 'computing', 'concepts', 'conference', 'connected', 'connectionism', 'contest', 'contests', 'contextdependent', 'convolution', 'convolutional', 'convolved', 'cortex', 'could', 'create', 'created', 'data', 'deal', 'dean', 'deep', 'define', 'deformation', 'demonstrated', 'deployed', 'described', 'design', 'designs', 'despite', 'developed', 'developmental', 'devices', 'didn', 'digital', 'digits', 'discovered', 'discovery', 'distributed', 'dns', 'document', 'domains', 'done', 'down', 'drastically', 'drugs', 'duda', 'each', 'earlier', 'effectively', 'efficient', 'eight', 'electron', 'em', 'embodied', 'embodiments', 'enough', 'equivalent', 'error', 'errors', 'et', 'even', 'evolved', 'example', 'exclusiveor', 'expanded', 'expert', 'exponentially', 'extended', 'face', 'far', 'farley', 'feasible', 'feature', 'feedforward', 'filter', 'filters', 'final', 'find', 'finetuned', 'finite', 'first', 'focused', 'followed', 'for', 'from', 'fully', 'functional', 'fundamentally', 'generative', 'gpubased', 'gpus', 'gradient', 'gradually', 'graves', 'greater', 'group', 'guarantee', 'habit', 'had', 'handle', 'handling', 'handwriting', 'handwritten', 'hardwarebased', 'has', 'have', 'hebb', 'hebbian', 'help', 'hidden', 'hierarchy', 'higherlevel', 'highlevel', 'highly', 'hinton', 'his', 'holland', 'however', 'hubel', 'humancompetitive', 'hypothesis', 'icdar', 'ideas', 'identify', 'ifthen', 'ijcnn', 'image', 'imagenet', 'images', 'impeding', 'implementations', 'in', 'incapable', 'including', 'increased', 'input', 'inspired', 'intelligence', 'interest', 'interfaced', 'international', 'into', 'introduced', 'invariance', 'is', 'isbi', 'issues', 'ivakhnenko', 'key', 'knowledge', 'known', 'lab', 'label', 'languages', 'lapa', 'large', 'largevocabulary', 'late', 'latent', 'laureates', 'layer', 'layers', 'learned', 'learning', 'least', 'led', 'level', 'lighting', 'linear', 'link', 'localization', 'location', 'logic', 'long', 'lowlevel', 'lstm', 'machine', 'machines', 'makes', 'making', 'many', 'manylayered', 'markov', 'mathematical', 'mathematics', 'maxpooling', 'may', 'mcclelland', 'mcculloch', 'mechanism', 'memory', 'merck', 'method', 'methods', 'microscopy', 'mid', 'might', 'minsky', 'mnist', 'model', 'models', 'modifying', 'molecules', 'much', 'multidimensional', 'multilayer', 'multilevel', 'name', 'nanodevices', 'natural', 'needed', 'neocognitron', 'nerve', 'network', 'networks', 'neural', 'neuromorphic', 'neuron', 'neuronal', 'new', 'ng', 'nobel', 'node', 'nonlinear', 'not', 'notation', 'number', 'object', 'objects', 'of', 'on', 'once', 'one', 'only', 'or', 'other', 'others', 'output', 'overcome', 'overtook', 'papert', 'parallel', 'parameters', 'particularly', 'pass', 'pattern', 'paved', 'perceptron', 'perceptrons', 'perform', 'performance', 'pitts', 'plasticity', 'pooling', 'popular', 'popularity', 'potentiation', 'power', 'prediction', 'pretrained', 'pretraining', 'primary', 'principal', 'prior', 'problem', 'problems', 'processed', 'processes', 'processing', 'propagate', 'proposed', 'protein', 'published', 'rather', 'realized', 'realvalued', 'recognition', 'recognize', 'recognizers', 'reconstruction', 'recurrent', 'reduce', 'relied', 'renewed', 'representation', 'reproducing', 'required', 'research', 'researchers', 'restricted', 'rnns', 'rochester', 'rosenblatt', 'rprop', 'rules', 'rumelhart', 'sampling', 'scale', 'scenes', 'schmidhuber', 'search', 'second', 'segmentation', 'several', 'shift', 'shortterm', 'showed', 'shown', 'shrink', 'sign', 'similar', 'simple', 'simpler', 'simulate', 'simulation', 'slowed', 'small', 'software', 'solved', 'some', 'sparsely', 'speech', 'split', 'sponsored', 'stacks', 'stagnated', 'standard', 'started', 'state', 'states', 'structures', 'subsymbolic', 'successfully', 'successive', 'such', 'sufficiently', 'superhuman', 'supervised', 'support', 'symbolic', 'systems', 'taken', 'tasks', 'team', 'term', 'than', 'that', 'the', 'their', 'then', 'there', 'these', 'they', 'this', 'those', 'though', 'three', 'threshold', 'through', 'time', 'to', 'tolerance', 'top', 'topped', 'traffic', 'trained', 'training', 'transformed', 'trigger', 'tuning', 'turing', 'two', 'type', 'types', 'under', 'unlabeled', 'unsupervised', 'until', 'up', 'use', 'used', 'using', 'usually', 'vanishing', 'variables', 'variants', 'vector', 'very', 'videos', 'vision', 'visual', 'voice', 'walter', 'warren', 'was', 'watching', 'way', 'weights', 'werbos', 'were', 'when', 'wherewhat', 'while', 'who', 'whose', 'wiesel', 'with', 'without', 'won', 'work', 'wwn', 'youtube']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "bag_of_words = count_vect.fit_transform(sents)\n",
    "print(count_vect.get_feature_names())\n",
    "print(bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.123639770109438\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(bag_of_words)\n",
    "tfidf_mat=tfidf.transform(bag_of_words)\n",
    "print(tfidf_mat[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Warren McCulloch and Walter Pitts    created a computational model for neural networks based on mathematics and algorithms called threshold logic.': 4.123639770109438, 'This model paved the way for neural network research to split into two approaches.': 3.5882741544327423, 'One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence.': 3.950432750011678, 'This work led to work on nerve networks and their link to finite automata.': 3.2130973433697747, 'Hebbian learning In the late  s  D. O. Hebb  created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning.': 3.9045646738813753, 'Hebbian learning is unsupervised learning.': 1.9745299544113393, 'This evolved into models for long term potentiation.': 2.795242211939756, 'Researchers started applying these ideas to computational models in   with Turing s Btype machines.': 3.4875855787727343, 'Farley and Clark    first used computational machines  then called  calculators  to simulate a Hebbian network.': 3.636492037407615, 'Other neural network computational machines were created by Rochester  Holland  Habit and Duda  .': 3.469373341586273, 'Rosenblatt    created the perceptron  an algorithm for pattern recognition.': 2.915335966119455, 'With mathematical notation  Rosenblatt described circuitry not in the basic perceptron  such as the exclusiveor circuit that could not be processed by neural networks at the time.': 4.568071954939416, 'In   a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex  simple cells and complex cells.': 4.579190401064462, 'The first functional networks with many layers were published by Ivakhnenko and Lapa in   becoming the Group Method of Data Handling.': 4.284558278274739, 'Neural network research stagnated after machine learning research by Minsky and Papert   who discovered two key issues with the computational machines that processed neural networks.': 4.552127983062961, 'The first was that basic perceptrons were incapable of processing the exclusiveor circuit.': 3.381337622722981, 'The second was that computers didn t have enough processing power to effectively handle the work required by large neural networks.': 4.213437033618936, 'Neural network research slowed until computers achieved far greater processing power.': 3.2524413897062563, 'Much of artificial intelligence had focused on highlevel  symbolic  models that are processed by using algorithms  characterized for example by expert systems with knowledge embodied in ifthen rules  until in the late  s research expanded to lowlevel  subsymbolic  machine learning  characterized by knowledge embodied in the parameters of a cognitive model.': 5.948048882424531, 'citation needed Backpropagation A key trigger for renewed interest in neural networks and learning was Werbos s   backpropagation algorithm that effectively solved the exclusiveor problem by making the training of multilayer networks feasible and efficient.': 5.194894035249142, 'Backpropagation distributed the error term back up through the layers  by modifying the weights at each node.': 3.8219161678577027, 'In the mid s  parallel distributed processing became popular under the name connectionism.': 3.2500855580182484, 'Rumelhart and McClelland   described the use of connectionism to simulate neural processes.': 3.2583977632006715, 'Support vector machines and other  much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity.': 4.284020970877231, 'However  using neural networks transformed some domains  such as the prediction of protein structures.': 3.5460184032545166, 'In   maxpooling was introduced to help with least shift invariance and tolerance to deformation to aid in  D object recognition.': 3.8852271488661554, 'In   Backpropagation training through maxpooling was accelerated by GPUs and shown to perform better than other pooling variants.': 4.095661671559163, 'The vanishing gradient problem affects manylayered feedforward networks that used backpropagation and also recurrent neural networks  RNNs .': 3.8495384230553658, 'As errors propagate from layer to layer  they shrink exponentially with the number of layers  impeding the tuning of neuron weights that is based on those errors  particularly affecting deep networks.': 4.919065494355367, 'To overcome this problem  Schmidhuber adopted a multilevel hierarchy of networks   pretrained one level at a time by unsupervised learning and finetuned by backpropagation.': 4.424531464884103, 'Behnke   relied only on the sign of the gradient  Rprop  on problems such as image reconstruction and face localization.': 3.99363859426464, 'Hinton et al.': 1.7320508075688772, 'proposed learning a highlevel representation using successive layers of binary or realvalued latent variables with a restricted Boltzmann machine  to model each layer.': 4.470395350565424, 'Once sufficiently many layers have been learned  the deep architecture may be used as a generative model by reproducing the data when sampling down the model  an  ancestral pass  from the top level feature activations.': 5.373498197659432, 'In   Ng and Dean created a network that learned to recognize higherlevel concepts  such as cats  only from watching unlabeled images taken from YouTube videos.': 4.568580619130348, 'Earlier challenges in training deep neural networks were successfully addressed with methods such as unsupervised pretraining  while available computing power increased through the use of GPUs and distributed computing.': 4.975276031404155, 'Neural networks were deployed on a large scale  particularly in image and visual recognition problems.': 3.574179841904951, 'This became known as  deep learning .': 2.402063648672309, 'citation needed Hardwarebased designs Computational devices were created in CMOS  for both biophysical simulation and neuromorphic computing.': 3.9986792378131777, 'Nanodevices  for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital  even though the first implementations may use digital devices .': 5.336514822205149, 'Ciresan and colleagues   in Schmidhuber s group showed that despite the vanishing gradient problem  GPUs makes backpropagation feasible for manylayered feedforward neural networks.': 4.5023388822332535, 'Contests Between   and   recurrent neural networks and deep feedforward neural networks developed in Schmidhuber s research group won eight international competitions in pattern recognition and machine learning.': 4.525927579209892, 'For example  the bidirectional and multidimensional long shortterm memory  LSTM  of Graves et al.': 3.5890343988344475, 'won three competitions in connected handwriting recognition at the   International Conference on Document Analysis and Recognition  ICDAR  without any prior knowledge about the three languages to be learned.': 4.731486324359963, 'Ciresan and colleagues won pattern recognition contests  including the IJCNN   Traffic Sign Recognition Competition  the ISBI   Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge  and others.': 4.8086089211250265, 'Their neural networks were the first pattern recognizers to achieve humancompetitive or even superhuman performance  on benchmarks such as traffic sign recognition  IJCNN   or the MNIST handwritten digits problem.': 4.961445884060319, 'Researchers demonstrated   that deep neural networks interfaced to a hidden Markov model with contextdependent states that define the neural network output layer can drastically reduce errors in largevocabulary speech recognition tasks such as voice search.': 5.4662192304478765, 'GPUbased implementations  of this approach won many pattern recognition contests  including the IJCNN   Traffic Sign Recognition Competition  the ISBI   Segmentation of neuronal structures in EM stacks challenge  the ImageNet Competition  and others.': 5.006391115600957, 'Deep  highly nonlinear neural architectures similar to the neocognitron  and the  standard architecture of vision  inspired by simple and complex cells  were pretrained by unsupervised methods by Hinton.': 4.720387884856768, 'A team from his lab won a   contest sponsored by Merck to design software to help find molecules that might identify new drugs.': 4.413828805497733, 'Convolutional networks As of   the state of the art in deep learning feedforward networks alternated between convolutional layers and maxpooling layers  topped by several fully or sparsely connected layers followed by a final classification layer.': 4.853926123260688, 'Learning is usually done without unsupervised pretraining.': 2.606184291355235, 'In the convolutional layer  there are filters that are convolved with the input.': 3.1220209559187833, 'Each filter is equivalent to a weights vector that has to be trained.': 3.2769021122805166, 'Such supervised deep learning methods were the first to achieve humancompetitive performance on certain tasks.': 3.7348427293413673, 'Artificial neural networks were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes  only when invariance extended beyond shift  to all ANNlearned concepts  such as location  type  object class label  scale  lighting and others.': 5.693364990315593, 'This was realized in Developmental Networks  DNs  whose embodiments are WhereWhat Networks  WWN    through WWN   .': 3.3766257224915477}\n"
     ]
    }
   ],
   "source": [
    "my_dict={}\n",
    "feature_name=count_vect.get_feature_names()\n",
    "for idx in range(len(sents)):\n",
    "    my_dict[sents[idx]]=0\n",
    "    for word in word_tokenize(sents[idx]):\n",
    "        if word.lower() in feature_name:\n",
    "            my_dict[sents[idx]]=tfidf_mat.todense()[idx].sum()\n",
    "\n",
    "print(my_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Much of artificial intelligence had focused on highlevel  symbolic  models that are processed by using algorithms  characterized for example by expert systems with knowledge embodied in ifthen rules  until in the late  s research expanded to lowlevel  subsymbolic  machine learning  characterized by knowledge embodied in the parameters of a cognitive model.', 5.948048882424531), ('Artificial neural networks were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes  only when invariance extended beyond shift  to all ANNlearned concepts  such as location  type  object class label  scale  lighting and others.', 5.693364990315593), ('Researchers demonstrated   that deep neural networks interfaced to a hidden Markov model with contextdependent states that define the neural network output layer can drastically reduce errors in largevocabulary speech recognition tasks such as voice search.', 5.4662192304478765), ('Once sufficiently many layers have been learned  the deep architecture may be used as a generative model by reproducing the data when sampling down the model  an  ancestral pass  from the top level feature activations.', 5.373498197659432), ('Nanodevices  for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital  even though the first implementations may use digital devices .', 5.336514822205149), ('citation needed Backpropagation A key trigger for renewed interest in neural networks and learning was Werbos s   backpropagation algorithm that effectively solved the exclusiveor problem by making the training of multilayer networks feasible and efficient.', 5.194894035249142), ('GPUbased implementations  of this approach won many pattern recognition contests  including the IJCNN   Traffic Sign Recognition Competition  the ISBI   Segmentation of neuronal structures in EM stacks challenge  the ImageNet Competition  and others.', 5.006391115600957), ('Earlier challenges in training deep neural networks were successfully addressed with methods such as unsupervised pretraining  while available computing power increased through the use of GPUs and distributed computing.', 4.975276031404155), ('Their neural networks were the first pattern recognizers to achieve humancompetitive or even superhuman performance  on benchmarks such as traffic sign recognition  IJCNN   or the MNIST handwritten digits problem.', 4.961445884060319), ('As errors propagate from layer to layer  they shrink exponentially with the number of layers  impeding the tuning of neuron weights that is based on those errors  particularly affecting deep networks.', 4.919065494355367), ('Convolutional networks As of   the state of the art in deep learning feedforward networks alternated between convolutional layers and maxpooling layers  topped by several fully or sparsely connected layers followed by a final classification layer.', 4.853926123260688), ('Ciresan and colleagues won pattern recognition contests  including the IJCNN   Traffic Sign Recognition Competition  the ISBI   Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge  and others.', 4.8086089211250265), ('won three competitions in connected handwriting recognition at the   International Conference on Document Analysis and Recognition  ICDAR  without any prior knowledge about the three languages to be learned.', 4.731486324359963), ('Deep  highly nonlinear neural architectures similar to the neocognitron  and the  standard architecture of vision  inspired by simple and complex cells  were pretrained by unsupervised methods by Hinton.', 4.720387884856768), ('In   a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex  simple cells and complex cells.', 4.579190401064462), ('In   Ng and Dean created a network that learned to recognize higherlevel concepts  such as cats  only from watching unlabeled images taken from YouTube videos.', 4.568580619130348), ('With mathematical notation  Rosenblatt described circuitry not in the basic perceptron  such as the exclusiveor circuit that could not be processed by neural networks at the time.', 4.568071954939416), ('Neural network research stagnated after machine learning research by Minsky and Papert   who discovered two key issues with the computational machines that processed neural networks.', 4.552127983062961), ('Contests Between   and   recurrent neural networks and deep feedforward neural networks developed in Schmidhuber s research group won eight international competitions in pattern recognition and machine learning.', 4.525927579209892), ('Ciresan and colleagues   in Schmidhuber s group showed that despite the vanishing gradient problem  GPUs makes backpropagation feasible for manylayered feedforward neural networks.', 4.5023388822332535), ('proposed learning a highlevel representation using successive layers of binary or realvalued latent variables with a restricted Boltzmann machine  to model each layer.', 4.470395350565424), ('To overcome this problem  Schmidhuber adopted a multilevel hierarchy of networks   pretrained one level at a time by unsupervised learning and finetuned by backpropagation.', 4.424531464884103), ('A team from his lab won a   contest sponsored by Merck to design software to help find molecules that might identify new drugs.', 4.413828805497733), ('The first functional networks with many layers were published by Ivakhnenko and Lapa in   becoming the Group Method of Data Handling.', 4.284558278274739), ('Support vector machines and other  much simpler methods such as linear classifiers gradually overtook neural networks in machine learning popularity.', 4.284020970877231), ('The second was that computers didn t have enough processing power to effectively handle the work required by large neural networks.', 4.213437033618936), ('Warren McCulloch and Walter Pitts    created a computational model for neural networks based on mathematics and algorithms called threshold logic.', 4.123639770109438), ('In   Backpropagation training through maxpooling was accelerated by GPUs and shown to perform better than other pooling variants.', 4.095661671559163), ('citation needed Hardwarebased designs Computational devices were created in CMOS  for both biophysical simulation and neuromorphic computing.', 3.9986792378131777), ('Behnke   relied only on the sign of the gradient  Rprop  on problems such as image reconstruction and face localization.', 3.99363859426464), ('One approach focused on biological processes in the brain while the other focused on the application of neural networks to artificial intelligence.', 3.950432750011678), ('Hebbian learning In the late  s  D. O. Hebb  created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning.', 3.9045646738813753), ('In   maxpooling was introduced to help with least shift invariance and tolerance to deformation to aid in  D object recognition.', 3.8852271488661554), ('The vanishing gradient problem affects manylayered feedforward networks that used backpropagation and also recurrent neural networks  RNNs .', 3.8495384230553658), ('Backpropagation distributed the error term back up through the layers  by modifying the weights at each node.', 3.8219161678577027), ('Such supervised deep learning methods were the first to achieve humancompetitive performance on certain tasks.', 3.7348427293413673), ('Farley and Clark    first used computational machines  then called  calculators  to simulate a Hebbian network.', 3.636492037407615), ('For example  the bidirectional and multidimensional long shortterm memory  LSTM  of Graves et al.', 3.5890343988344475), ('This model paved the way for neural network research to split into two approaches.', 3.5882741544327423), ('Neural networks were deployed on a large scale  particularly in image and visual recognition problems.', 3.574179841904951), ('However  using neural networks transformed some domains  such as the prediction of protein structures.', 3.5460184032545166), ('Researchers started applying these ideas to computational models in   with Turing s Btype machines.', 3.4875855787727343), ('Other neural network computational machines were created by Rochester  Holland  Habit and Duda  .', 3.469373341586273), ('The first was that basic perceptrons were incapable of processing the exclusiveor circuit.', 3.381337622722981), ('This was realized in Developmental Networks  DNs  whose embodiments are WhereWhat Networks  WWN    through WWN   .', 3.3766257224915477), ('Each filter is equivalent to a weights vector that has to be trained.', 3.2769021122805166), ('Rumelhart and McClelland   described the use of connectionism to simulate neural processes.', 3.2583977632006715), ('Neural network research slowed until computers achieved far greater processing power.', 3.2524413897062563), ('In the mid s  parallel distributed processing became popular under the name connectionism.', 3.2500855580182484), ('This work led to work on nerve networks and their link to finite automata.', 3.2130973433697747), ('In the convolutional layer  there are filters that are convolved with the input.', 3.1220209559187833), ('Rosenblatt    created the perceptron  an algorithm for pattern recognition.', 2.915335966119455), ('This evolved into models for long term potentiation.', 2.795242211939756), ('Learning is usually done without unsupervised pretraining.', 2.606184291355235), ('This became known as  deep learning .', 2.402063648672309), ('Hebbian learning is unsupervised learning.', 1.9745299544113393), ('Hinton et al.', 1.7320508075688772)]\n"
     ]
    }
   ],
   "source": [
    "sorted_by_value = sorted(my_dict.items(), key=lambda kv: kv[1],reverse=True)\n",
    "print(sorted_by_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Much of artificial intelligence had focused on highlevel  symbolic  models that are processed by using algorithms  characterized for example by expert systems with knowledge embodied in ifthen rules  until in the late  s research expanded to lowlevel  subsymbolic  machine learning  characterized by knowledge embodied in the parameters of a cognitive model.\n",
      "Artificial neural networks were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes  only when invariance extended beyond shift  to all ANNlearned concepts  such as location  type  object class label  scale  lighting and others.\n",
      "Researchers demonstrated   that deep neural networks interfaced to a hidden Markov model with contextdependent states that define the neural network output layer can drastically reduce errors in largevocabulary speech recognition tasks such as voice search.\n",
      "Once sufficiently many layers have been learned  the deep architecture may be used as a generative model by reproducing the data when sampling down the model  an  ancestral pass  from the top level feature activations.\n",
      "Nanodevices  for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital  even though the first implementations may use digital devices .\n"
     ]
    }
   ],
   "source": [
    "#Summary 2 , number of sentences=5\n",
    "\n",
    "for sent ,value in sorted_by_value[0:5]:\n",
    "    print(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
